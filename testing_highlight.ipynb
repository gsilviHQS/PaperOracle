{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to highlight part of a pdf file, use the following command:\n",
    "# !python -m pip install --upgrade pymupdf\n",
    "import fitz\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gsilvi/miniforge3/envs/PaperOracle/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from functions import extract_all_text2\n",
    "import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tex = extract_all_text2('papers/2101.00689/Npi_paper.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYS: dict_keys([])\n",
      "SECTIONS: dict_keys(['-', 'author', 'affiliation', 'title', 'date', 'abstract', 'Introduction', 'Gauge Ensemble', 'Interpolating Operators', 'Wick contractions', 'Spectra results', 'L{\\\\\"u}scher quantization conditions}\\n', 'Results for the scattering amplitudes', 'Parametrizations used', 'Fit procedure and results', 'Conclusions', 'Acknowledgments', 'One-to-one mapping of energy levels to phase shifts in irreps without mixing between $J=1/2$ and $J=3/2$', 'Transformation properties of operators', '\\\\texorpdfstring{Matrices $\\\\mathcalM'])\n",
      "sec='-'\n",
      "lines=[57, 62, 65, 69, 70, 74, 78, 81, 84, 87, 90, 93, 94, 98, 103, 106, 116, 118, 119, 120]\n",
      "sec='author'\n",
      "lines=[58, 63, 66, 71, 75, 79, 82, 85, 88, 91, 95, 99]\n",
      "sec='affiliation'\n",
      "lines=[60, 61, 64, 67, 68, 72, 73, 76, 77, 80, 83, 86, 89, 92, 96, 97, 100, 101, 102]\n",
      "sec='title'\n",
      "lines=[104]\n",
      "sec='date'\n",
      "lines=[105]\n",
      "sec='abstract'\n",
      "lines=[108]\n",
      "sec='Introduction'\n",
      "lines=[123]\n",
      "sec='Gauge Ensemble'\n",
      "lines=[150]\n",
      "KEYWORD: \\end{table} 153\n",
      "Closing KEYWORD: \\end{table} 173\n",
      "sec='Interpolating Operators'\n",
      "lines=[195]\n",
      "KEYWORD: \\end{table*} 198\n",
      "Closing KEYWORD: \\end{table*} 223\n",
      "KEYWORD: \\end{table*} 278\n",
      "Closing KEYWORD: \\end{table*} 323\n",
      "sec='Wick contractions'\n",
      "lines=[400]\n",
      "KEYWORD: \\end{figure} 401\n",
      "Closing KEYWORD: \\end{figure} 410\n",
      "sec='Spectra results'\n",
      "lines=[437]\n",
      "KEYWORD: \\end{figure} 439\n",
      "Closing KEYWORD: \\end{figure} 442\n",
      "KEYWORD: \\end{figure} 445\n",
      "Closing KEYWORD: \\end{figure} 448\n",
      "KEYWORD: \\end{table} 452\n",
      "Closing KEYWORD: \\end{table} 489\n",
      "KEYWORD: \\end{figure*} 492\n",
      "Closing KEYWORD: \\end{figure*} 499\n",
      "KEYWORD: \\end{figure*} 501\n",
      "Closing KEYWORD: \\end{figure*} 507\n",
      "KEYWORD: \\end{figure*} 510\n",
      "Closing KEYWORD: \\end{figure*} 515\n",
      "sec='L{\\\\\"u}scher quantization conditions}\\n'\n",
      "lines=[599]\n",
      "KEYWORD: \\end{table*} 623\n",
      "Closing KEYWORD: \\end{table*} 645\n",
      "sec='Results for the scattering amplitudes'\n",
      "lines=[714]\n",
      "KEYWORD: \\end{table*} 718\n",
      "Closing KEYWORD: \\end{table*} 746\n",
      "sec='Parametrizations used'\n",
      "lines=[752]\n",
      "sec='Fit procedure and results'\n",
      "lines=[781]\n",
      "KEYWORD: \\end{figure*} 783\n",
      "Closing KEYWORD: \\end{figure*} 788\n",
      "KEYWORD: \\end{table*} 792\n",
      "Closing KEYWORD: \\end{table*} 813\n",
      "sec='Conclusions'\n",
      "lines=[893]\n",
      "sec='Acknowledgments'\n",
      "lines=[906]\n",
      "sec='One-to-one mapping of energy levels to phase shifts in irreps without mixing between $J=1/2$ and $J=3/2$'\n",
      "lines=[916]\n",
      "KEYWORD: \\end{table} 921\n",
      "Closing KEYWORD: \\end{table} 948\n",
      "sec='Transformation properties of operators'\n",
      "lines=[953]\n",
      "sec='\\\\texorpdfstring{Matrices $\\\\mathcalM'\n",
      "lines=[991]\n",
      "KEYWORD: \\end{footnotesize} 997\n",
      "Closing KEYWORD: \\end{footnotesize} 1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1698 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase too long: 3368  at lines: [203, 205, 210, 212, 214, 216, 219, 221]\n",
      "Splitting phrase in  1  parts\n",
      "Phrase too long: 5290  at lines: [285, 286, 288, 289, 291, 292, 293, 294, 295, 297, 298, 299, 301, 302, 303, 304, 306, 307, 308, 309, 310, 312, 313, 314, 316, 317, 318]\n",
      "Splitting phrase in  1  parts\n",
      "Phrase too long: 3647  at lines: [456, 458, 459, 461, 462, 464, 465, 466, 468, 469, 471, 472, 473, 474, 475, 477, 478, 479, 481, 482, 484, 485]\n",
      "Splitting phrase in  1  parts\n",
      "Phrase too long: 2365  at lines: [601, 602, 608, 609, 614, 618, 621]\n",
      "Splitting phrase in  1  parts\n",
      "Phrase too long: 2089  at lines: [627, 631, 635, 637, 641]\n",
      "Splitting phrase in  1  parts\n",
      "Phrase too long: 4059  at lines: [722, 727, 730, 733, 736, 739, 742]\n",
      "Splitting phrase in  1  parts\n",
      "Phrase too long: 2041  at lines: [798, 799, 800, 801, 804, 806]\n",
      "Splitting phrase in  1  parts\n",
      "Phrase too long: 2446  at lines: [925, 927, 928, 930, 931, 933, 934, 936, 937, 940, 942, 943]\n",
      "Splitting phrase in  1  parts\n",
      "Phrase too long: 8690  at lines: [999, 1008, 1009, 1011, 1021, 1022, 1023, 1026, 1027, 1036, 1039]\n",
      "Splitting phrase in  1  parts\n"
     ]
    }
   ],
   "source": [
    "final_text = embedding_functions.texStripper2(all_tex,'title','abstract') # refine the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for file in final_text.keys():\n",
    "    df = pd.DataFrame(final_text[file]['full'], columns=['Phrase','Lines'])\n",
    "\n",
    "# fix the fact that \\begin{} and end{} should not be asked in SyncTex, Which means when lines start with \n",
    "# \\begin{ or \\end{, they should not be added to the list of lines to be highlighted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "# papers/2101.00689/Npi_paper.tex\n",
    "def call_syncTex(dataframe, path_to_pdf, path_to_tex):\n",
    "    # add a colum Page to the dataframe\n",
    "    dataframe['Page'] = None\n",
    "    \n",
    "    gen_dict = {}\n",
    "    lines_in_Tex = []\n",
    "    for i in range(len(df)):\n",
    "        collection = []\n",
    "        lines_in_Tex = df.loc[i,'Lines']\n",
    "        \n",
    "        \n",
    "        for row in lines_in_Tex:\n",
    "            # print(f\"{row=}\")\n",
    "            result = subprocess.run(['synctex', \n",
    "                                    'view',\n",
    "                                    '-i',f'{row}:0:{path_to_tex}',\n",
    "                                    '-o',f'{path_to_pdf}' ], \n",
    "                                    stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "            # print(result)\n",
    "            \n",
    "            # print(f\"{result=}\")\n",
    "            \n",
    "            for item in result.split('\\n'):\n",
    "                if ':' in item:\n",
    "                    key, value = item.split(':')\n",
    "                    gen_dict[key] = value\n",
    "                    gen_dict['TeX_line'] = row\n",
    "                    if key=='after':\n",
    "                        # if 228<=gen_dict['TeX_line']<=230:\n",
    "                        collection.append(gen_dict)\n",
    "                        gen_dict = {}\n",
    "\n",
    "            \n",
    "            # print(f\"{collection=} {row=}\")\n",
    "        text_instances = set()\n",
    "        for stuck in collection:\n",
    "            if float(stuck['W'])>0:\n",
    "                if float(stuck['y'])<float(stuck['v']):\n",
    "                    text_instances.add((stuck['Page'],\n",
    "                                        float(stuck['h']),\n",
    "                                        # float(stuck['v'])-float(stuck['H']),\n",
    "                                        float(stuck['y'])-9,\n",
    "                                        # max(float(stuck['y'])-9,float(stuck['v'])-float(stuck['H'])),\n",
    "                                        float(stuck['h'])+float(stuck['W']),\n",
    "                                        float(stuck['v']))\n",
    "                                        )\n",
    "                else:\n",
    "                    text_instances.add((stuck['Page'],\n",
    "                                        float(stuck['h']),\n",
    "                                        float(stuck['v'])-9,\n",
    "                                        float(stuck['h'])+float(stuck['W']),\n",
    "                                        float(stuck['v']))\n",
    "                                )\n",
    "        for ii, inst in enumerate(text_instances):\n",
    "            # divide the tuple inst into its components\n",
    "            # print(f\"{inst=}\")\n",
    "            (pageNum, x0, y0, x1, y1) = inst \n",
    "            # if it already exists, append pageNum and the 4 coordinates as a list to the dataframe\n",
    "            if df.loc[i,'Page'] != None:\n",
    "                df.loc[i,'Page'].append(pageNum)\n",
    "                df.loc[i,'Highlight'].append(str([x0, y0, x1, y1]))\n",
    "            # if it does not exist, create a new row in the dataframe\n",
    "            else:\n",
    "                df.loc[i,'Page'] = [pageNum]\n",
    "                df.loc[i,'Highlight'] = [str([x0, y0, x1, y1])]\n",
    "    return df\n",
    "\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = call_syncTex(df, 'papers/2101.00689/Npi_paper.pdf', 'papers/2101.00689/Npi_paper.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the dataframe and highlight the text\n",
    "doc = fitz.open('papers/2101.00689/Npi_paper.pdf')\n",
    "for i in range(len(df)):\n",
    "    # skip the NaN values\n",
    "    if type(df.loc[i,'Page'])==list:\n",
    "        for j in range(len(df.loc[i,'Page'])):\n",
    "            page = doc[int(df.loc[i,'Page'][j])-1]\n",
    "            rectangle = fitz.Rect(eval(df.loc[i,'Highlight'][j]))\n",
    "            highlight = page.add_highlight_annot(rectangle)\n",
    "            highlight.update()\n",
    "\n",
    "doc.save('papers/2101.00689/Npi_paper_highlighted.pdf', garbage=4, deflate=True, clean=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "doc = fitz.open(\"papers/2101.00689/Npi_paper.pdf\")\n",
    "for page in doc:\n",
    "    ### SEARCH\n",
    "    print(page.number)\n",
    "    \n",
    "    for ii, inst in enumerate(df):\n",
    "        \n",
    "        # divide the tuple inst into its components\n",
    "        (pageNum, x0, y0, x1, y1) = inst \n",
    "        if pageNum==str(page.number+1):\n",
    "            highlight = page.add_highlight_annot(fitz.Rect(x0, y0, x1, y1))\n",
    "            highlight.update()\n",
    "    # for inst in text_instances2:\n",
    "   \n",
    "    #     # divide the tuple inst into its components\n",
    "    #     (pageNum, x0, y0, x1, y1) = inst \n",
    "    #     if pageNum==str(page.number+1):\n",
    "    #         print(inst)\n",
    "    #         highlight = page.add_underline_annot(fitz.Rect(x0, y0, x1, y1))\n",
    "    #         highlight.update()\n",
    "### OUTPUT\n",
    "\n",
    "doc.save(\"output.pdf\", garbage=4, deflate=True, clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[131, 132]\n",
      "This is SyncTeX command line utility, version 1.5\n",
      "SyncTeX result begin\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:320.376617\n",
      "y:662.376038\n",
      "h:314.966400\n",
      "v:664.866699\n",
      "W:225.033615\n",
      "H:9.962640\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:321.663513\n",
      "y:616.547913\n",
      "h:314.966400\n",
      "v:619.038574\n",
      "W:225.033615\n",
      "H:9.962640\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:320.501190\n",
      "y:673.833069\n",
      "h:314.966400\n",
      "v:676.323730\n",
      "W:225.033615\n",
      "H:9.962640\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:337.382355\n",
      "y:605.090881\n",
      "h:314.966400\n",
      "v:607.028076\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:340.454193\n",
      "y:650.919006\n",
      "h:314.966400\n",
      "v:653.409668\n",
      "W:225.033615\n",
      "H:9.962640\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:332.124298\n",
      "y:628.004944\n",
      "h:314.966400\n",
      "v:629.942139\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:403.108276\n",
      "y:559.262756\n",
      "h:314.966400\n",
      "v:561.199951\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:345.158783\n",
      "y:570.719788\n",
      "h:314.966400\n",
      "v:572.656982\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:329.107819\n",
      "y:582.176819\n",
      "h:314.966400\n",
      "v:584.114014\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:344.051849\n",
      "y:593.633850\n",
      "h:314.966400\n",
      "v:595.571045\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:341.865540\n",
      "y:639.461975\n",
      "h:314.966400\n",
      "v:641.399170\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:358.691406\n",
      "y:685.290100\n",
      "h:314.966400\n",
      "v:687.227295\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "SyncTeX result end\n",
      "\n",
      "This is SyncTeX command line utility, version 1.5\n",
      "SyncTeX result begin\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:362.621124\n",
      "y:696.747192\n",
      "h:314.966400\n",
      "v:699.237854\n",
      "W:225.033615\n",
      "H:9.962640\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:391.736908\n",
      "y:685.290100\n",
      "h:314.966400\n",
      "v:687.227295\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "SyncTeX result end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#set the environment variable TOKENIZERS_PARALLELISM=true\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# extract line 36 from df\n",
    "import subprocess\n",
    "# line_in_df = 15\n",
    "# get all Lines from df\n",
    "lines = []\n",
    "for i in range(len(df)):\n",
    "    lines.extend(df.loc[i,'Lines'])\n",
    "# print(lines)\n",
    "\n",
    "result = ''\n",
    "collection = list()\n",
    "gen_dict = {}\n",
    "\n",
    "line_in_df = 11\n",
    "lines = df.iloc[line_in_df].Lines\n",
    "print(lines)\n",
    "for row in lines:\n",
    "    # print('Row',row)\n",
    "    result = subprocess.run(['synctex', 'view', '-i',f'{row}:0:papers/2101.00689/Npi_paper.tex','-o','papers/2101.00689/Npi_paper.pdf' ], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(result)\n",
    "    for stuck in result.split('\\n'):\n",
    "        \n",
    "        if ':' in stuck:\n",
    "            key, value = stuck.split(':')\n",
    "            gen_dict[key] = value\n",
    "            gen_dict['TeX_line'] = row\n",
    "            if key=='after':\n",
    "                # if 228<=gen_dict['TeX_line']<=230:\n",
    "                collection.append(gen_dict)\n",
    "                gen_dict = {}\n",
    "    \n",
    "import pprint\n",
    "# pprint.pprint(collection)\n",
    "\n",
    "# height from y to v\n",
    "# width from x to h+W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h v represent the beginning of the line x and y\n",
    "# x0<x1 , y0<y1, increase number means go down or right\n",
    "\n",
    "#0._______\n",
    "# |       | \n",
    "# |_______.1\n",
    "\n",
    "\n",
    "### READ IN PDF\n",
    "doc = fitz.open(\"papers/2101.00689/Npi_paper.pdf\")\n",
    "# text_instances = [fitz.Rect(132,489, 296,500)]\n",
    "text_instances = set()\n",
    "text_instances2 = set()\n",
    "\n",
    "for stuck in collection:\n",
    "    #yellow\n",
    "    if float(stuck['W'])>0:\n",
    "        if float(stuck['y'])<float(stuck['v']):\n",
    "            text_instances.add((stuck['Page'],\n",
    "                                float(stuck['h']),\n",
    "                                # float(stuck['v'])-float(stuck['H']),\n",
    "                                float(stuck['y'])-9,\n",
    "                                # max(float(stuck['y'])-9,float(stuck['v'])-float(stuck['H'])),\n",
    "                                float(stuck['h'])+float(stuck['W']),\n",
    "                                float(stuck['v']))\n",
    "                                )\n",
    "        else:\n",
    "            text_instances.add((stuck['Page'],\n",
    "                                float(stuck['h']),\n",
    "                                float(stuck['v'])-9,\n",
    "                                float(stuck['h'])+float(stuck['W']),\n",
    "                                float(stuck['v']))\n",
    "                            )\n",
    "    # #green\n",
    "    # #if float(stuck['y'])<float(stuck['v']):\n",
    "    # text_instances.add((stuck['Page'],\n",
    "    #                         float(stuck['x'])-10,\n",
    "    #                         float(stuck['y'])-10,\n",
    "    #                         float(stuck['x']),\n",
    "    #                         float(stuck['y']),\n",
    "    #                         )\n",
    "    #                         )\n",
    "    # text_instances.add((stuck['Page'],\n",
    "    #                         float(stuck['h']),\n",
    "    #                         float(stuck['v'])-float(stuck['H']),\n",
    "    #                         float(stuck['x']),\n",
    "    #                         float(stuck['v']),\n",
    "    #                         )\n",
    "    #                         )\n",
    "    \n",
    "    ### HIGHLIGHT\n",
    "for page in doc:\n",
    "    ### SEARCH\n",
    "    print(page.number)\n",
    "    \n",
    "    for ii, inst in enumerate(text_instances):\n",
    "        \n",
    "        # divide the tuple inst into its components\n",
    "        (pageNum, x0, y0, x1, y1) = inst \n",
    "        if pageNum==str(page.number+1):\n",
    "            highlight = page.add_highlight_annot(fitz.Rect(x0, y0, x1, y1))\n",
    "            highlight.update()\n",
    "    # for inst in text_instances2:\n",
    "   \n",
    "    #     # divide the tuple inst into its components\n",
    "    #     (pageNum, x0, y0, x1, y1) = inst \n",
    "    #     if pageNum==str(page.number+1):\n",
    "    #         print(inst)\n",
    "    #         highlight = page.add_underline_annot(fitz.Rect(x0, y0, x1, y1))\n",
    "    #         highlight.update()\n",
    "### OUTPUT\n",
    "\n",
    "doc.save(\"output.pdf\", garbage=4, deflate=True, clean=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('PaperOracle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adfda680b332929768398405ffef5c5fca2eb1802e013d485b0c57cfe5351235"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
