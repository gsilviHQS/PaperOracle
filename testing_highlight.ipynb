{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to highlight part of a pdf file, use the following command:\n",
    "# !python -m pip install --upgrade pymupdf\n",
    "import fitz\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gsilvi/miniforge3/envs/PaperOracle/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from functions import extract_all_text2\n",
    "import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tex = extract_all_text2('papers/2101.00689/Npi_paper.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYS: dict_keys([])\n",
      "SECTIONS: dict_keys(['-', 'author', 'affiliation', 'title', 'date', 'abstract', 'Introduction', 'Gauge Ensemble', 'Interpolating Operators', 'Wick contractions', 'Spectra results', 'L{\\\\\"u}scher quantization conditions}\\n', 'Results for the scattering amplitudes', 'Parametrizations used', 'Fit procedure and results', 'Conclusions', 'Acknowledgments', 'One-to-one mapping of energy levels to phase shifts in irreps without mixing between $J=1/2$ and $J=3/2$', 'Transformation properties of operators', '\\\\texorpdfstring{Matrices $\\\\mathcalM'])\n",
      "sec='-'\n",
      "lines=[57, 62, 65, 69, 70, 74, 78, 81, 84, 87, 90, 93, 94, 98, 103, 106, 116, 118, 119, 120]\n",
      "sec='author'\n",
      "lines=[58, 63, 66, 71, 75, 79, 82, 85, 88, 91, 95, 99]\n",
      "sec='affiliation'\n",
      "lines=[60, 61, 64, 67, 68, 72, 73, 76, 77, 80, 83, 86, 89, 92, 96, 97, 100, 101, 102]\n",
      "sec='title'\n",
      "lines=[104]\n",
      "sec='date'\n",
      "lines=[105]\n",
      "sec='abstract'\n",
      "lines=[108]\n",
      "sec='Introduction'\n",
      "lines=[123]\n",
      "sec='Gauge Ensemble'\n",
      "lines=[150]\n",
      "KEYWORD: \\end{table} 153\n",
      "Closing KEYWORD: \\end{table} 173\n",
      "sec='Interpolating Operators'\n",
      "lines=[195]\n",
      "KEYWORD: \\end{table*} 198\n",
      "Closing KEYWORD: \\end{table*} 223\n",
      "KEYWORD: \\end{table*} 278\n",
      "Closing KEYWORD: \\end{table*} 323\n",
      "sec='Wick contractions'\n",
      "lines=[400]\n",
      "KEYWORD: \\end{figure} 401\n",
      "Closing KEYWORD: \\end{figure} 410\n",
      "sec='Spectra results'\n",
      "lines=[437]\n",
      "KEYWORD: \\end{figure} 439\n",
      "Closing KEYWORD: \\end{figure} 442\n",
      "KEYWORD: \\end{figure} 445\n",
      "Closing KEYWORD: \\end{figure} 448\n",
      "KEYWORD: \\end{table} 452\n",
      "Closing KEYWORD: \\end{table} 489\n",
      "KEYWORD: \\end{figure*} 492\n",
      "Closing KEYWORD: \\end{figure*} 499\n",
      "KEYWORD: \\end{figure*} 501\n",
      "Closing KEYWORD: \\end{figure*} 507\n",
      "KEYWORD: \\end{figure*} 510\n",
      "Closing KEYWORD: \\end{figure*} 515\n",
      "sec='L{\\\\\"u}scher quantization conditions}\\n'\n",
      "lines=[599]\n",
      "KEYWORD: \\end{table*} 623\n",
      "Closing KEYWORD: \\end{table*} 645\n",
      "sec='Results for the scattering amplitudes'\n",
      "lines=[714]\n",
      "KEYWORD: \\end{table*} 718\n",
      "Closing KEYWORD: \\end{table*} 746\n",
      "sec='Parametrizations used'\n",
      "lines=[752]\n",
      "sec='Fit procedure and results'\n",
      "lines=[781]\n",
      "KEYWORD: \\end{figure*} 783\n",
      "Closing KEYWORD: \\end{figure*} 788\n",
      "KEYWORD: \\end{table*} 792\n",
      "Closing KEYWORD: \\end{table*} 813\n",
      "sec='Conclusions'\n",
      "lines=[893]\n",
      "sec='Acknowledgments'\n",
      "lines=[906]\n",
      "sec='One-to-one mapping of energy levels to phase shifts in irreps without mixing between $J=1/2$ and $J=3/2$'\n",
      "lines=[916]\n",
      "KEYWORD: \\end{table} 921\n",
      "Closing KEYWORD: \\end{table} 948\n",
      "sec='Transformation properties of operators'\n",
      "lines=[953]\n",
      "sec='\\\\texorpdfstring{Matrices $\\\\mathcalM'\n",
      "lines=[991]\n",
      "KEYWORD: \\end{footnotesize} 997\n",
      "Closing KEYWORD: \\end{footnotesize} 1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1698 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase too long: 3368  at lines: [203, 205, 210, 212, 214, 216, 219, 221]\n",
      "Splitting phrase in  1  parts\n",
      "Phrase too long: 5290  at lines: [285, 286, 288, 289, 291, 292, 293, 294, 295, 297, 298, 299, 301, 302, 303, 304, 306, 307, 308, 309, 310, 312, 313, 314, 316, 317, 318]\n",
      "Splitting phrase in  1  parts\n",
      "Phrase too long: 3647  at lines: [456, 458, 459, 461, 462, 464, 465, 466, 468, 469, 471, 472, 473, 474, 475, 477, 478, 479, 481, 482, 484, 485]\n",
      "Splitting phrase in  1  parts\n",
      "Phrase too long: 4059  at lines: [722, 727, 730, 733, 736, 739, 742]\n",
      "Splitting phrase in  1  parts\n",
      "Phrase too long: 8690  at lines: [999, 1008, 1009, 1011, 1021, 1022, 1023, 1026, 1027, 1036, 1039]\n",
      "Splitting phrase in  1  parts\n"
     ]
    }
   ],
   "source": [
    "final_text = embedding_functions.texStripper2(all_tex,'title','abstract') # refine the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for file in final_text.keys():\n",
    "    df = pd.DataFrame(final_text[file]['full'], columns=['Phrase','Lines'])\n",
    "\n",
    "# fix the fact that \\begin{} and end{} should not be asked in SyncTex, Which means when lines start with \n",
    "# \\begin{ or \\end{, they should not be added to the list of lines to be highlighted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import math\n",
    "# papers/2101.00689/Npi_paper.tex\n",
    "def call_syncTex(dataframe, path_to_pdf, path_to_tex):\n",
    "    # add a colum Page to the dataframe\n",
    "    dataframe['Page'] = None\n",
    "    \n",
    "    gen_dict = {}\n",
    "    lines_in_Tex = []\n",
    "    for i in range(len(df)):\n",
    "        collection = []\n",
    "        lines_in_Tex = df.loc[i,'Lines']\n",
    "        \n",
    "        \n",
    "        for row in lines_in_Tex:\n",
    "            # print(f\"{row=}\")\n",
    "            result = subprocess.run(['synctex', \n",
    "                                    'view',\n",
    "                                    '-i',f'{row}:0:{path_to_tex}',\n",
    "                                    '-o',f'{path_to_pdf}' ], \n",
    "                                    stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "            # print(result)\n",
    "            \n",
    "            # print(f\"{result=}\")\n",
    "            \n",
    "            for item in result.split('\\n'):\n",
    "                if ':' in item:\n",
    "                    key, value = item.split(':')\n",
    "                    gen_dict[key] = value\n",
    "                    gen_dict['TeX_line'] = row\n",
    "                    if key=='after':\n",
    "                        # if 228<=gen_dict['TeX_line']<=230:\n",
    "                        collection.append(gen_dict)\n",
    "                        gen_dict = {}\n",
    "\n",
    "            \n",
    "            # print(f\"{collection=} {row=}\")\n",
    "        pre_text_instances = set()\n",
    "        text_instances = set()\n",
    "        \n",
    "        for stuck in collection:\n",
    "            page = stuck['Page']\n",
    "            x0 = math.ceil(float(stuck['h']))\n",
    "            y0 = math.ceil(float(stuck['y'])-9)\n",
    "            x1 = math.ceil(float(stuck['h'])+float(stuck['W']))\n",
    "            y1 = math.ceil(float(stuck['v']))\n",
    "            # print(text_instances)\n",
    "            if float(stuck['W'])>0 and float(stuck['y'])<float(stuck['v']):\n",
    "                    pre_text_instances.add((page,x0,y0,x1,y1))\n",
    "\n",
    "       \n",
    "        # text_instances.add((page,x0,y0,x1,y1))\n",
    "        for p in pre_text_instances:\n",
    "            new_tuple = p\n",
    "            for s in pre_text_instances:\n",
    "                if s[0]==new_tuple[0] and (s[1]==new_tuple[1] or s[2]==new_tuple[2] or s[3]==new_tuple[3] or s[4]==new_tuple[4]):\n",
    "                    new_tuple = (new_tuple[0],\n",
    "                                    min(s[1],new_tuple[1]),\n",
    "                                    min(s[2],new_tuple[2]),\n",
    "                                    max(s[3],new_tuple[3]),\n",
    "                                    max(s[4],new_tuple[4]))\n",
    "            if not any(new_tuple[0]==t[0] and (new_tuple[1]==t[1] or new_tuple[2]==t[2] or new_tuple[3]==t[3] or new_tuple[4]==t[4]) for t in text_instances):\n",
    "                text_instances.add(new_tuple)\n",
    "\n",
    "        for ii, inst in enumerate(text_instances):\n",
    "            # divide the tuple inst into its components\n",
    "            # print(f\"{inst=}\")\n",
    "            (pageNum, x0, y0, x1, y1) = inst \n",
    "            # if it already exists, append pageNum and the 4 coordinates as a list to the dataframe\n",
    "            if df.loc[i,'Page'] != None:\n",
    "                # check if the page is already in the list\n",
    "                \n",
    "                df.loc[i,'Page'].append(pageNum)\n",
    "                df.loc[i,'Highlight'].append(str([x0, y0, x1, y1]))\n",
    "            # if it does not exist, create a new row in the dataframe\n",
    "            else:\n",
    "                df.loc[i,'Page'] = [pageNum]\n",
    "                df.loc[i,'Highlight'] = [str([x0, y0, x1, y1])]\n",
    "    return df\n",
    "\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_instances={('1', 72, 334, 540, 356)}\n",
      "text_instances={('1', 72, 334, 540, 356)}\n",
      "text_instances=set()\n",
      "text_instances=set()\n",
      "text_instances={('1', 72, 334, 540, 356)}\n",
      "text_instances={('1', 72, 352, 540, 377)}\n",
      "text_instances={('1', 72, 366, 540, 398)}\n",
      "text_instances={('1', 72, 387, 540, 430)}\n",
      "text_instances={('1', 72, 418, 540, 471)}\n",
      "text_instances={('1', 72, 517, 298, 645)}\n",
      "text_instances={('1', 72, 645, 298, 679), ('1', 315, 492, 541, 549)}\n",
      "text_instances={('1', 315, 551, 541, 700)}\n",
      "text_instances={('1', 315, 702, 541, 722), ('2', 72, 73, 298, 303)}\n",
      "text_instances={('2', 72, 303, 298, 475)}\n",
      "text_instances={('2', 72, 476, 298, 624)}\n",
      "text_instances={('2', 315, 68, 541, 178)}\n",
      "text_instances={('2', 315, 219, 541, 310), ('2', 72, 689, 298, 723)}\n",
      "text_instances={('2', 315, 322, 541, 368)}\n",
      "text_instances={('2', 315, 357, 541, 402)}\n",
      "text_instances={('2', 315, 391, 541, 425)}\n",
      "text_instances={('2', 315, 425, 541, 483)}\n",
      "text_instances={('2', 315, 530, 541, 576)}\n",
      "text_instances={('3', 72, 302, 298, 524), ('2', 313, 577, 541, 672)}\n",
      "text_instances={('3', 72, 514, 298, 559)}\n",
      "text_instances={('3', 72, 548, 298, 583)}\n",
      "text_instances={('3', 72, 569, 298, 617)}\n",
      "text_instances={('3', 72, 629, 298, 664)}\n",
      "text_instances={('3', 72, 652, 298, 700)}\n",
      "text_instances={('3', 315, 302, 541, 336), ('3', 72, 689, 298, 723)}\n",
      "text_instances={('3', 315, 325, 541, 359)}\n",
      "text_instances={('3', 315, 348, 541, 370)}\n",
      "text_instances={('3', 404, 422, 440, 456), ('3', 371, 425, 403, 452), ('3', 315, 382, 549, 600)}\n",
      "text_instances={('3', 377, 631, 409, 659), ('4', 72, 467, 298, 479), ('3', 364, 628, 532, 680), ('3', 315, 600, 542, 723)}\n",
      "text_instances={('4', 72, 467, 298, 517)}\n",
      "text_instances={('4', 72, 505, 298, 528)}\n",
      "text_instances={('4', 368, 479, 540, 592), ('4', 72, 540, 298, 723)}\n",
      "text_instances={('5', 145, 129, 180, 163), ('4', 72, 458, 540, 720), ('5', 72, 73, 298, 212)}\n",
      "text_instances={('5', 72, 212, 298, 236)}\n",
      "text_instances={('5', 72, 224, 298, 271)}\n",
      "text_instances={('5', 72, 271, 298, 342)}\n",
      "text_instances={('5', 72, 330, 298, 353)}\n",
      "text_instances={('5', 315, 397, 541, 575)}\n",
      "text_instances={('5', 72, 401, 298, 528)}\n",
      "text_instances={('5', 72, 517, 298, 540)}\n",
      "text_instances={('5', 72, 528, 298, 631)}\n",
      "text_instances={('5', 72, 620, 298, 654)}\n",
      "text_instances={('5', 72, 654, 298, 688)}\n",
      "text_instances={('5', 72, 677, 298, 711)}\n",
      "text_instances={('5', 315, 643, 541, 723)}\n",
      "text_instances=set()\n",
      "text_instances=set()\n",
      "text_instances=set()\n",
      "text_instances={('7', 72, 487, 540, 561)}\n",
      "text_instances=set()\n",
      "text_instances={('9', 72, 283, 540, 325)}\n",
      "text_instances={('6', 72, 493, 298, 515)}\n",
      "text_instances={('6', 72, 504, 298, 630)}\n",
      "text_instances={('6', 72, 631, 298, 724), ('6', 315, 402, 541, 419)}\n",
      "text_instances={('6', 315, 406, 541, 507)}\n",
      "text_instances={('6', 315, 507, 541, 628)}\n",
      "text_instances={('6', 315, 617, 541, 651)}\n",
      "text_instances={('6', 315, 651, 541, 688)}\n",
      "text_instances={('7', 72, 583, 298, 652), ('6', 315, 677, 541, 720)}\n",
      "text_instances={('7', 315, 583, 541, 595), ('7', 72, 666, 298, 722)}\n",
      "text_instances={('7', 315, 583, 541, 722), ('8', 72, 532, 298, 608), ('7', 369, 634, 487, 652)}\n",
      "text_instances={('8', 315, 532, 541, 565), ('8', 72, 597, 298, 723)}\n",
      "text_instances={('8', 315, 566, 541, 649)}\n",
      "text_instances={('8', 315, 638, 541, 688)}\n",
      "text_instances={('8', 315, 689, 541, 723)}\n",
      "text_instances={('9', 72, 345, 298, 382), ('8', 315, 711, 541, 723)}\n",
      "text_instances={('9', 72, 371, 298, 406)}\n",
      "text_instances={('9', 72, 417, 298, 440)}\n",
      "text_instances={('9', 465, 522, 502, 535), ('9', 315, 348, 541, 573), ('9', 444, 478, 473, 516), ('9', 72, 480, 517, 724)}\n",
      "text_instances={('10', 141, 69, 472, 188), ('10', 72, 174, 540, 189)}\n",
      "text_instances={('9', 315, 560, 545, 660), ('10', 72, 226, 298, 269)}\n",
      "text_instances={('10', 131, 477, 299, 505), ('10', 228, 474, 251, 486), ('10', 72, 292, 309, 551)}\n",
      "text_instances={('10', 72, 551, 298, 608)}\n",
      "text_instances={('10', 72, 608, 298, 700)}\n",
      "text_instances={('10', 72, 689, 298, 722)}\n",
      "text_instances={('10', 315, 231, 541, 253)}\n",
      "text_instances={('10', 315, 254, 541, 334)}\n",
      "text_instances={('10', 315, 334, 541, 380)}\n",
      "text_instances={('10', 315, 369, 541, 483)}\n",
      "text_instances={('10', 315, 472, 541, 507)}\n",
      "text_instances={('10', 315, 601, 543, 712), ('11', 97, 565, 131, 578), ('10', 392, 630, 540, 645), ('11', 72, 336, 298, 653), ('11', 147, 356, 222, 367)}\n",
      "text_instances={('13', 72, 603, 540, 686)}\n",
      "text_instances={('12', 345, 83, 440, 158)}\n",
      "text_instances={('11', 72, 700, 298, 723), ('11', 315, 336, 541, 507)}\n",
      "text_instances={('11', 315, 495, 541, 551)}\n",
      "text_instances={('11', 315, 540, 541, 573)}\n",
      "text_instances={('11', 315, 574, 541, 631)}\n",
      "text_instances={('11', 315, 631, 541, 677)}\n",
      "text_instances={('11', 315, 666, 541, 712)}\n",
      "text_instances={('12', 72, 207, 298, 222), ('11', 315, 700, 541, 723)}\n",
      "text_instances={('12', 72, 211, 298, 245)}\n",
      "text_instances={('12', 72, 245, 298, 302)}\n",
      "text_instances={('12', 72, 291, 298, 337)}\n",
      "text_instances={('12', 72, 326, 298, 452)}\n",
      "text_instances={('12', 72, 440, 298, 474)}\n",
      "text_instances={('12', 72, 463, 298, 485)}\n",
      "text_instances={('12', 315, 211, 541, 337), ('12', 72, 497, 298, 722)}\n",
      "text_instances={('12', 315, 348, 541, 634), ('12', 373, 593, 483, 603)}\n",
      "text_instances={('12', 362, 711, 540, 723), ('12', 315, 646, 541, 703), ('14', 72, 73, 298, 108)}\n",
      "text_instances={('14', 72, 159, 298, 353)}\n",
      "text_instances={('14', 72, 365, 298, 413)}\n",
      "text_instances={('14', 72, 413, 298, 619)}\n",
      "text_instances={('14', 72, 631, 298, 711)}\n",
      "text_instances={('14', 315, 73, 541, 154), ('14', 72, 711, 298, 722)}\n",
      "text_instances={('14', 315, 216, 541, 261)}\n",
      "text_instances={('14', 315, 250, 541, 353)}\n",
      "text_instances={('14', 315, 342, 541, 399)}\n",
      "text_instances={('14', 315, 388, 541, 559)}\n",
      "text_instances=set()\n",
      "text_instances={('15', 315, 73, 541, 84), ('14', 315, 643, 541, 722), ('15', 72, 348, 298, 485)}\n",
      "text_instances={('15', 72, 93, 298, 225), ('15', 86, 77, 284, 224), ('15', 133, 70, 145, 197), ('15', 104, 94, 108, 114)}\n",
      "text_instances={('15', 315, 144, 541, 178)}\n",
      "text_instances={('15', 356, 349, 500, 369), ('15', 315, 144, 541, 402)}\n",
      "text_instances={('15', 315, 402, 541, 474)}\n",
      "text_instances=set()\n",
      "text_instances={('15', 72, 555, 540, 596)}\n",
      "text_instances=set()\n"
     ]
    }
   ],
   "source": [
    "df = call_syncTex(df, 'papers/2101.00689/Npi_paper.pdf', 'papers/2101.00689/Npi_paper.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the dataframe and highlight the text\n",
    "doc = fitz.open('papers/2101.00689/Npi_paper.pdf')\n",
    "for i in range(len(df)):\n",
    "    # skip the NaN values\n",
    "    if type(df.loc[i,'Page'])==list:\n",
    "        for j in range(len(df.loc[i,'Page'])):\n",
    "            page = doc[int(df.loc[i,'Page'][j])-1]\n",
    "            rectangle = fitz.Rect(eval(df.loc[i,'Highlight'][j]))\n",
    "            highlight = page.add_highlight_annot(rectangle)\n",
    "            highlight.update()\n",
    "\n",
    "doc.save('papers/2101.00689/Npi_paper_highlighted.pdf', garbage=4, deflate=True, clean=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "doc = fitz.open(\"papers/2101.00689/Npi_paper.pdf\")\n",
    "for page in doc:\n",
    "    ### SEARCH\n",
    "    print(page.number)\n",
    "    \n",
    "    for ii, inst in enumerate(df):\n",
    "        \n",
    "        # divide the tuple inst into its components\n",
    "        (pageNum, x0, y0, x1, y1) = inst \n",
    "        if pageNum==str(page.number+1):\n",
    "            highlight = page.add_highlight_annot(fitz.Rect(x0, y0, x1, y1))\n",
    "            highlight.update()\n",
    "    # for inst in text_instances2:\n",
    "   \n",
    "    #     # divide the tuple inst into its components\n",
    "    #     (pageNum, x0, y0, x1, y1) = inst \n",
    "    #     if pageNum==str(page.number+1):\n",
    "    #         print(inst)\n",
    "    #         highlight = page.add_underline_annot(fitz.Rect(x0, y0, x1, y1))\n",
    "    #         highlight.update()\n",
    "### OUTPUT\n",
    "\n",
    "doc.save(\"output.pdf\", garbage=4, deflate=True, clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[131, 132]\n",
      "This is SyncTeX command line utility, version 1.5\n",
      "SyncTeX result begin\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:320.376617\n",
      "y:662.376038\n",
      "h:314.966400\n",
      "v:664.866699\n",
      "W:225.033615\n",
      "H:9.962640\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:321.663513\n",
      "y:616.547913\n",
      "h:314.966400\n",
      "v:619.038574\n",
      "W:225.033615\n",
      "H:9.962640\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:320.501190\n",
      "y:673.833069\n",
      "h:314.966400\n",
      "v:676.323730\n",
      "W:225.033615\n",
      "H:9.962640\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:337.382355\n",
      "y:605.090881\n",
      "h:314.966400\n",
      "v:607.028076\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:340.454193\n",
      "y:650.919006\n",
      "h:314.966400\n",
      "v:653.409668\n",
      "W:225.033615\n",
      "H:9.962640\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:332.124298\n",
      "y:628.004944\n",
      "h:314.966400\n",
      "v:629.942139\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:403.108276\n",
      "y:559.262756\n",
      "h:314.966400\n",
      "v:561.199951\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:345.158783\n",
      "y:570.719788\n",
      "h:314.966400\n",
      "v:572.656982\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:329.107819\n",
      "y:582.176819\n",
      "h:314.966400\n",
      "v:584.114014\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:344.051849\n",
      "y:593.633850\n",
      "h:314.966400\n",
      "v:595.571045\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:341.865540\n",
      "y:639.461975\n",
      "h:314.966400\n",
      "v:641.399170\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:358.691406\n",
      "y:685.290100\n",
      "h:314.966400\n",
      "v:687.227295\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "SyncTeX result end\n",
      "\n",
      "This is SyncTeX command line utility, version 1.5\n",
      "SyncTeX result begin\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:362.621124\n",
      "y:696.747192\n",
      "h:314.966400\n",
      "v:699.237854\n",
      "W:225.033615\n",
      "H:9.962640\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "Output:papers/2101.00689/Npi_paper.pdf\n",
      "Page:1\n",
      "x:391.736908\n",
      "y:685.290100\n",
      "h:314.966400\n",
      "v:687.227295\n",
      "W:225.033615\n",
      "H:8.855677\n",
      "before:\n",
      "offset:-1\n",
      "middle:\n",
      "after:\n",
      "SyncTeX result end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#set the environment variable TOKENIZERS_PARALLELISM=true\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# extract line 36 from df\n",
    "import subprocess\n",
    "# line_in_df = 15\n",
    "# get all Lines from df\n",
    "lines = []\n",
    "for i in range(len(df)):\n",
    "    lines.extend(df.loc[i,'Lines'])\n",
    "# print(lines)\n",
    "\n",
    "result = ''\n",
    "collection = list()\n",
    "gen_dict = {}\n",
    "\n",
    "line_in_df = 11\n",
    "lines = df.iloc[line_in_df].Lines\n",
    "print(lines)\n",
    "for row in lines:\n",
    "    # print('Row',row)\n",
    "    result = subprocess.run(['synctex', 'view', '-i',f'{row}:0:papers/2101.00689/Npi_paper.tex','-o','papers/2101.00689/Npi_paper.pdf' ], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    print(result)\n",
    "    for stuck in result.split('\\n'):\n",
    "        \n",
    "        if ':' in stuck:\n",
    "            key, value = stuck.split(':')\n",
    "            gen_dict[key] = value\n",
    "            gen_dict['TeX_line'] = row\n",
    "            if key=='after':\n",
    "                # if 228<=gen_dict['TeX_line']<=230:\n",
    "                collection.append(gen_dict)\n",
    "                gen_dict = {}\n",
    "    \n",
    "import pprint\n",
    "# pprint.pprint(collection)\n",
    "\n",
    "# height from y to v\n",
    "# width from x to h+W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h v represent the beginning of the line x and y\n",
    "# x0<x1 , y0<y1, increase number means go down or right\n",
    "\n",
    "#0._______\n",
    "# |       | \n",
    "# |_______.1\n",
    "\n",
    "\n",
    "### READ IN PDF\n",
    "doc = fitz.open(\"papers/2101.00689/Npi_paper.pdf\")\n",
    "# text_instances = [fitz.Rect(132,489, 296,500)]\n",
    "text_instances = set()\n",
    "text_instances2 = set()\n",
    "\n",
    "for stuck in collection:\n",
    "    #yellow\n",
    "    if float(stuck['W'])>0:\n",
    "        if float(stuck['y'])<float(stuck['v']):\n",
    "            text_instances.add((stuck['Page'],\n",
    "                                float(stuck['h']),\n",
    "                                # float(stuck['v'])-float(stuck['H']),\n",
    "                                float(stuck['y'])-9,\n",
    "                                # max(float(stuck['y'])-9,float(stuck['v'])-float(stuck['H'])),\n",
    "                                float(stuck['h'])+float(stuck['W']),\n",
    "                                float(stuck['v']))\n",
    "                                )\n",
    "        else:\n",
    "            text_instances.add((stuck['Page'],\n",
    "                                float(stuck['h']),\n",
    "                                float(stuck['v'])-9,\n",
    "                                float(stuck['h'])+float(stuck['W']),\n",
    "                                float(stuck['v']))\n",
    "                            )\n",
    "    # #green\n",
    "    # #if float(stuck['y'])<float(stuck['v']):\n",
    "    # text_instances.add((stuck['Page'],\n",
    "    #                         float(stuck['x'])-10,\n",
    "    #                         float(stuck['y'])-10,\n",
    "    #                         float(stuck['x']),\n",
    "    #                         float(stuck['y']),\n",
    "    #                         )\n",
    "    #                         )\n",
    "    # text_instances.add((stuck['Page'],\n",
    "    #                         float(stuck['h']),\n",
    "    #                         float(stuck['v'])-float(stuck['H']),\n",
    "    #                         float(stuck['x']),\n",
    "    #                         float(stuck['v']),\n",
    "    #                         )\n",
    "    #                         )\n",
    "    \n",
    "    ### HIGHLIGHT\n",
    "for page in doc:\n",
    "    ### SEARCH\n",
    "    print(page.number)\n",
    "    \n",
    "    for ii, inst in enumerate(text_instances):\n",
    "        \n",
    "        # divide the tuple inst into its components\n",
    "        (pageNum, x0, y0, x1, y1) = inst \n",
    "        if pageNum==str(page.number+1):\n",
    "            highlight = page.add_highlight_annot(fitz.Rect(x0, y0, x1, y1))\n",
    "            highlight.update()\n",
    "    # for inst in text_instances2:\n",
    "   \n",
    "    #     # divide the tuple inst into its components\n",
    "    #     (pageNum, x0, y0, x1, y1) = inst \n",
    "    #     if pageNum==str(page.number+1):\n",
    "    #         print(inst)\n",
    "    #         highlight = page.add_underline_annot(fitz.Rect(x0, y0, x1, y1))\n",
    "    #         highlight.update()\n",
    "### OUTPUT\n",
    "\n",
    "doc.save(\"output.pdf\", garbage=4, deflate=True, clean=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('PaperOracle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adfda680b332929768398405ffef5c5fca2eb1802e013d485b0c57cfe5351235"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
